{
  "2": {
    "inputs": {
      "text": [
        "30",
        0
      ],
      "clip": [
        "60",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "3": {
    "inputs": {
      "text": [
        "31",
        0
      ],
      "clip": [
        "60",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "4": {
    "inputs": {
      "seed": 1019975081,
      "steps": [
        "13",
        0
      ],
      "cfg": [
        "13",
        1
      ],
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "60",
        0
      ],
      "positive": [
        "43",
        0
      ],
      "negative": [
        "43",
        1
      ],
      "latent_image": [
        "5",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "5": {
    "inputs": {
      "width": [
        "17",
        0
      ],
      "height": [
        "17",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "6": {
    "inputs": {
      "samples": [
        "4",
        0
      ],
      "vae": [
        "11",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "10": {
    "inputs": {
      "filename": "%time_%seed",
      "path": "%date",
      "extension": "png",
      "steps": [
        "13",
        0
      ],
      "cfg": [
        "13",
        1
      ],
      "modelname": [
        "11",
        3
      ],
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "positive": [
        "30",
        0
      ],
      "negative": [
        "31",
        0
      ],
      "seed_value": 190303188274,
      "width": [
        "17",
        0
      ],
      "height": [
        "17",
        1
      ],
      "lossless_webp": true,
      "quality_jpeg_or_webp": 100,
      "optimize_png": false,
      "counter": 0,
      "denoise": 1,
      "clip_skip": -2,
      "time_format": "%Y-%m-%d-%H%M%S",
      "save_workflow_as_json": false,
      "embed_workflow": true,
      "additional_hashes": "",
      "download_civitai_data": true,
      "easy_remix": true,
      "images": [
        "73",
        0
      ]
    },
    "class_type": "Image Saver",
    "_meta": {
      "title": "Image Saver"
    }
  },
  "11": {
    "inputs": {
      "ckpt_name": "prefectIllustriousXL_v10.safetensors"
    },
    "class_type": "Checkpoint Loader with Name (Image Saver)",
    "_meta": {
      "title": "Checkpoint Loader with Name (Image Saver)"
    }
  },
  "13": {
    "inputs": {
      "steps": 30,
      "cfg": 5.000000000000001
    },
    "class_type": "StepsAndCfg",
    "_meta": {
      "title": "Steps & Cfg"
    }
  },
  "17": {
    "inputs": {
      "Width": 1152,
      "Height": 720,
      "Batch": 1,
      "Landscape": true,
      "HiResMultiplier": 1.5
    },
    "class_type": "CanvasCreatorAdvanced",
    "_meta": {
      "title": "Create Canvas Advanced"
    }
  },
  "25": {
    "inputs": {
      "resize_scale": [
        "17",
        5
      ],
      "resize_method": "nearest",
      "upscale_model": [
        "27",
        0
      ],
      "image": [
        "6",
        0
      ]
    },
    "class_type": "UpscaleImageByModelThenResize",
    "_meta": {
      "title": "Upscale Image By Model Then Resize"
    }
  },
  "27": {
    "inputs": {
      "model_name": "RealESRGAN_x4.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "30": {
    "inputs": {
      "text": "full body,full shot,standing,looking at viewer,(solo:1.2),simple background,(white background:1.2),hand on own hip,(white background:1.1), Phoebe \\(Wuthering Waves\\), clothes: Phoebe def white skirt and whit hat, white see-through pantyhose, 1girl, solo, gloves, long blonde hair, long hair, black gloves, blue eyes, white boots, A full-body portrait of a young girl,standing gracefully,pastel art style,high-key lighting,shoujo manga influence,hd quality,delicate details,"
    },
    "class_type": "TextBoxMira",
    "_meta": {
      "title": "Text Box"
    }
  },
  "31": {
    "inputs": {
      "text": "bad quality,worst quality,worst detail,sketch,lowres,bad anatomy,blurry,(worst quality:1.8),low quality,hands bad,(normal quality:1.3),bad hands,mutated hands and fingers,extra legs,extra arms,duplicate,cropped,jpeg,artifacts,long body,multiple breasts,mutated,disfigured,bad proportions,bad feet,ugly,missing limb,monochrome,face bad"
    },
    "class_type": "TextBoxMira",
    "_meta": {
      "title": "Text Box"
    }
  },
  "32": {
    "inputs": {
      "clip_name": "clip_vision_g.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "36": {
    "inputs": {
      "image": "options.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "39": {
    "inputs": {
      "strength": 1.0000000000000002,
      "noise_augmentation": 0,
      "conditioning": [
        "2",
        0
      ],
      "clip_vision_output": [
        "42",
        0
      ]
    },
    "class_type": "unCLIPConditioning",
    "_meta": {
      "title": "unCLIPConditioning"
    }
  },
  "41": {
    "inputs": {
      "pixels": [
        "36",
        0
      ],
      "vae": [
        "11",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "42": {
    "inputs": {
      "crop": "center",
      "clip_vision": [
        "32",
        0
      ],
      "image": [
        "36",
        0
      ]
    },
    "class_type": "CLIPVisionEncode",
    "_meta": {
      "title": "CLIP Vision Encode"
    }
  },
  "43": {
    "inputs": {
      "strength": 1.0000000000000002,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "39",
        0
      ],
      "negative": [
        "3",
        0
      ],
      "control_net": [
        "45",
        0
      ],
      "image": [
        "48",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "44": {
    "inputs": {
      "image": "2025-05-28-024632_190303188274.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "45": {
    "inputs": {
      "control_net_name": "diffusion_pytorch_model_promax.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "46": {
    "inputs": {
      "images": [
        "48",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "48": {
    "inputs": {
      "low_threshold": 100,
      "high_threshold": 200,
      "resolution": 192,
      "image": [
        "50",
        0
      ]
    },
    "class_type": "CannyEdgePreprocessor",
    "_meta": {
      "title": "Canny Edge"
    }
  },
  "49": {
    "inputs": {
      "ckpt_name": "depth_anything_v2_vitl.pth",
      "resolution": 1472,
      "image": [
        "57",
        0
      ]
    },
    "class_type": "DepthAnythingV2Preprocessor",
    "_meta": {
      "title": "Depth Anything V2 - Relative"
    }
  },
  "50": {
    "inputs": {
      "detect_hand": "enable",
      "detect_body": "enable",
      "detect_face": "enable",
      "resolution": 1024,
      "scale_stick_for_xinsr_cn": "disable",
      "image": [
        "49",
        0
      ]
    },
    "class_type": "OpenposePreprocessor",
    "_meta": {
      "title": "OpenPose Pose"
    }
  },
  "57": {
    "inputs": {
      "merge_with_lineart": "lineart_anime",
      "resolution": 1152,
      "lineart_lower_bound": 0,
      "lineart_upper_bound": 1,
      "object_min_size": 36,
      "object_connectivity": 1,
      "image": [
        "44",
        0
      ]
    },
    "class_type": "AnyLineArtPreprocessor_aux",
    "_meta": {
      "title": "AnyLine Lineart"
    }
  },
  "58": {
    "inputs": {
      "images": [
        "6",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "60": {
    "inputs": {
      "lora_name": "illustrious\\Dark_Fantasy_anime_illus.safetensors",
      "strength_model": 0.7100000000000002,
      "strength_clip": 0.9900000000000002,
      "model": [
        "67",
        0
      ],
      "clip": [
        "67",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "63": {
    "inputs": {
      "lora_name": "illustrious\\yujingtang-ntrv4-v2.safetensors",
      "strength_model": 0.7800000000000001,
      "strength_clip": 1,
      "model": [
        "11",
        0
      ],
      "clip": [
        "11",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "64": {
    "inputs": {
      "lora_name": "illustrious\\Phoebe-ill-Tanger.safetensors",
      "strength_model": 0.8000000000000002,
      "strength_clip": 1,
      "model": [
        "63",
        0
      ],
      "clip": [
        "63",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "65": {
    "inputs": {
      "lora_name": "illustrious\\CunnyfunkyV4.1-000012.safetensors",
      "strength_model": 0.7300000000000001,
      "strength_clip": 1,
      "model": [
        "66",
        0
      ],
      "clip": [
        "66",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "66": {
    "inputs": {
      "lora_name": "illustrious\\c_cartethyia&fleurdelys  (wuthering waves)_nbe11_xl.safetensors",
      "strength_model": 1.0000000000000002,
      "strength_clip": 1,
      "model": [
        "64",
        0
      ],
      "clip": [
        "64",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "67": {
    "inputs": {
      "lora_name": "illustrious\\atreids-v2-style.safetensors",
      "strength_model": 0.12000000000000002,
      "strength_clip": 1,
      "model": [
        "68",
        0
      ],
      "clip": [
        "68",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "68": {
    "inputs": {
      "lora_name": "illustrious\\yujingtang-ntrv4-v2.safetensors",
      "strength_model": 0.6900000000000002,
      "strength_clip": 1,
      "model": [
        "65",
        0
      ],
      "clip": [
        "65",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "70": {
    "inputs": {
      "tile_size": 512,
      "overlap": 64,
      "temporal_size": 64,
      "temporal_overlap": 8,
      "pixels": [
        "25",
        0
      ],
      "vae": [
        "11",
        2
      ]
    },
    "class_type": "VAEEncodeTiled",
    "_meta": {
      "title": "VAE Encode (Tiled)"
    }
  },
  "71": {
    "inputs": {
      "seed": 1019975081,
      "steps": 20,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "11",
        0
      ],
      "positive": [
        "2",
        0
      ],
      "negative": [
        "3",
        0
      ],
      "latent_image": [
        "70",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "72": {
    "inputs": {
      "tile_size": 512,
      "overlap": 64,
      "temporal_size": 64,
      "temporal_overlap": 8,
      "samples": [
        "71",
        0
      ],
      "vae": [
        "11",
        2
      ]
    },
    "class_type": "VAEDecodeTiled",
    "_meta": {
      "title": "VAE Decode (Tiled)"
    }
  },
  "73": {
    "inputs": {
      "method": "Mean",
      "src_image": [
        "72",
        0
      ],
      "ref_image": [
        "6",
        0
      ]
    },
    "class_type": "ImageColorTransferMira",
    "_meta": {
      "title": "Color Transfer"
    }
  }
}